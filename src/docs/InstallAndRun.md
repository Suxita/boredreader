# Ollama - рЃўрЃюрЃАрЃбрЃљрЃџрЃљрЃфрЃўрЃљ рЃЊрЃљ рЃњрЃљрЃЏрЃЮрЃДрЃћрЃюрЃћрЃЉрЃљ


## рЃўрЃюрЃАрЃбрЃљрЃџрЃљрЃфрЃўрЃљ
рЃњрЃљрЃЊрЃЏрЃЮрЃгрЃћрЃарЃљ [рЃЮрЃцрЃўрЃфрЃўрЃљрЃџрЃБрЃарЃў рЃАрЃљрЃўрЃбрЃўрЃЊрЃљрЃю](https://ollama.com/download)
### macOS-рЃќрЃћ
```bash
# Homebrew-рЃўрЃА рЃњрЃљрЃЏрЃЮрЃДрЃћрЃюрЃћрЃЉрЃўрЃЌ
brew install ollama

curl -fsSL https://ollama.com/install.sh | sh
```

### Windows-рЃќрЃћ
1. рЃњрЃљрЃЊрЃЏрЃЮрЃўрЃгрЃћрЃарЃћрЃЌ `ollama-setup.exe` рЃЮрЃцрЃўрЃфрЃўрЃљрЃџрЃБрЃарЃў рЃАрЃљрЃўрЃбрЃўрЃЊрЃљрЃю
2. рЃЊрЃљрЃљрЃЏрЃљрЃбрЃћрЃЌ PATH-рЃерЃў ollama.exe-рЃА рЃЏрЃЊрЃћрЃЉрЃљрЃарЃћрЃЮрЃЉрЃљ

## рЃЏрЃЮрЃЊрЃћрЃџрЃћрЃЉрЃўрЃА рЃЕрЃљрЃЏрЃЮрЃбрЃЋрЃўрЃарЃЌрЃЋрЃљ

### рЃЏрЃЮрЃЊрЃћрЃџрЃўрЃА рЃЕрЃљрЃЏрЃЮрЃЌрЃЋрЃўрЃарЃЌрЃЋрЃљ:
```bash
ollama pull <Model name>
#рЃЏрЃљрЃњрЃљрЃџрЃўрЃЌрЃљрЃЊ
ollama pull Tinnyllama

```

## рЃФрЃўрЃарЃўрЃЌрЃљрЃЊрЃў рЃЉрЃарЃФрЃљрЃюрЃћрЃЉрЃћрЃЉрЃў

### рЃЏрЃЮрЃЊрЃћрЃџрЃўрЃА рЃњрЃљрЃерЃЋрЃћрЃЉрЃљ:
```bash
# рЃўрЃюрЃбрЃћрЃарЃљрЃЦрЃфрЃўрЃБрЃџрЃў рЃарЃћрЃЪрЃўрЃЏрЃў
ollama run llama2

# рЃЎрЃЮрЃюрЃЎрЃарЃћрЃбрЃБрЃџрЃў рЃерЃћрЃЎрЃўрЃЌрЃ«рЃЋрЃљ
ollama run llama2 "Explain artificial intelligence in simple terms"

```

### рЃЏрЃЮрЃЊрЃћрЃџрЃћрЃЉрЃўрЃА рЃЏрЃћрЃюрЃћрЃ»рЃЏрЃћрЃюрЃбрЃў:
```bash
# рЃДрЃЋрЃћрЃџрЃљ рЃЕрЃљрЃЏрЃЮрЃбрЃЋрЃўрЃарЃЌрЃБрЃџрЃў рЃЏрЃЮрЃЊрЃћрЃџрЃўрЃА рЃАрЃўрЃљ
ollama list

# рЃЏрЃЮрЃЊрЃћрЃџрЃўрЃА рЃгрЃљрЃерЃџрЃљ
ollama rm Tinnyllama

# рЃЏрЃЮрЃЊрЃћрЃџрЃўрЃА рЃўрЃюрЃцрЃЮрЃарЃЏрЃљрЃфрЃўрЃљ
ollama show Tinnyllama

# рЃЏрЃЮрЃЊрЃћрЃџрЃўрЃА рЃЕрЃљрЃЏрЃЮрЃбрЃЋрЃўрЃарЃЌрЃЋрЃљ рЃцрЃЮрЃюрЃерЃў
ollama pull Tinnyllama &
```

### рЃАрЃћрЃарЃЋрЃўрЃАрЃўрЃА рЃЏрЃћрЃюрЃћрЃ»рЃЏрЃћрЃюрЃбрЃў:
```bash
# рЃАрЃћрЃарЃЋрЃўрЃАрЃўрЃА рЃЊрЃљрЃгрЃДрЃћрЃЉрЃљ
ollama serve #рЃџрЃўрЃюрЃБрЃЦрЃАрЃћ рЃљрЃБрЃфрЃўрЃџрЃћрЃЉрЃћрЃџрЃў рЃЦрЃЮрЃЏрЃљрЃюрЃЊрЃў, рЃЋрЃўрЃюрЃЊрЃЮрЃБрЃАрЃќрЃћ рЃЎрЃў рЃљрЃарЃљ рЃарЃљрЃЊрЃњрЃљрЃю рЃЮрЃџрЃџрЃљрЃЏрЃљ рЃцрЃЮрЃюрЃБрЃа рЃарЃћрЃЪрЃўрЃЏрЃерЃў рЃўрЃЦрЃюрЃћ рЃњрЃљрЃерЃЋрЃћрЃЉрЃБрЃџрЃў рЃўрЃАрЃћрЃЊрЃљрЃф

# рЃАрЃћрЃарЃЋрЃўрЃАрЃўрЃА рЃерЃћрЃЕрЃћрЃарЃћрЃЉрЃљ
# Linux-рЃќрЃћ
sudo systemctl stop ollama

# macOS-рЃќрЃћ
brew services stop ollama
```


## рЃАрЃ«рЃЋрЃљрЃЊрЃљрЃАрЃ«рЃЋрЃљ рЃўрЃюрЃбрЃћрЃњрЃарЃљрЃфрЃўрЃљ

### VS Code Extension
1. рЃЊрЃљрЃљрЃўрЃюрЃАрЃбрЃљрЃџрЃўрЃарЃћрЃЌ "Ollama" extension VS Code-рЃерЃў
2. рЃњрЃљрЃ«рЃАрЃћрЃюрЃўрЃЌ Command Palette (`Ctrl+Shift+P`)
3. рЃЕрЃљрЃгрЃћрЃарЃћрЃЌ "Ollama" рЃЊрЃљ рЃњрЃљрЃЏрЃЮрЃўрЃДрЃћрЃюрЃћрЃЌ



## рЃЎрЃЮрЃюрЃцрЃўрЃњрЃБрЃарЃљрЃфрЃўрЃљ

### рЃФрЃўрЃарЃўрЃЌрЃљрЃЊрЃў рЃърЃљрЃарЃљрЃЏрЃћрЃбрЃарЃћрЃЉрЃў:
```bash
# рЃЏрЃЮрЃЊрЃћрЃџрЃўрЃА рЃбрЃћрЃЏрЃърЃћрЃарЃљрЃбрЃБрЃарЃљ­ЪїА№ИЈ (рЃерЃћрЃЏрЃЮрЃЦрЃЏрЃћрЃЊрЃћрЃЉрЃўрЃЌрЃЮрЃЉрЃљ)
ollama run llama2 --temperature 0.7

# рЃбрЃЮрЃЎрЃћрЃюрЃћрЃЉрЃўрЃА рЃЏрЃљрЃЦрЃА рЃарЃљрЃЮрЃЊрЃћрЃюрЃЮрЃЉрЃљ
ollama run llama2 --num-predict 1000

# рЃЎрЃЮрЃюрЃбрЃћрЃЦрЃАрЃбрЃўрЃА рЃАрЃўрЃњрЃарЃФрЃћ
ollama run llama2 --num-ctx 4096
```

### Modelfile-рЃўрЃА рЃерЃћрЃЦрЃЏрЃюрЃљ:
```
FROM llama2

# рЃАрЃўрЃАрЃбрЃћрЃЏрЃБрЃарЃў рЃърЃарЃЮрЃЏрЃърЃбрЃў
SYSTEM "You are a helpful assistant that speaks Georgian."

# рЃърЃљрЃарЃљрЃЏрЃћрЃбрЃарЃћрЃЉрЃў
PARAMETER temperature 0.8
PARAMETER num_predict 2000
PARAMETER num_ctx 4096

# рЃцрЃарЃљрЃќрЃўрЃА рЃерЃљрЃЉрЃџрЃЮрЃюрЃў
TEMPLATE """{{ if .System }}<|system|>
{{ .System }}<|end|>
{{ end }}{{ if .Prompt }}<|user|>
{{ .Prompt }}<|end|>
{{ end }}<|assistant|>
{{ .Response }}<|end|>
"""
```

рЃЏрЃЮрЃЊрЃћрЃџрЃўрЃА рЃерЃћрЃЦрЃЏрЃюрЃљ:
```bash
ollama create my-georgian-model -f ./Modelfile
ollama run my-georgian-model
```


## рЃљрЃџрЃбрЃћрЃарЃюрЃљрЃбрЃўрЃЋрЃћрЃЉрЃў

- **LM Studio**: GUI рЃўрЃюрЃбрЃћрЃарЃцрЃћрЃўрЃАрЃў
- **GPT4All**: рЃЏрЃАрЃњрЃљрЃЋрЃАрЃў рЃцрЃБрЃюрЃЦрЃфрЃўрЃЮрЃюрЃљрЃџрЃў
- **Llama.cpp**: рЃБрЃцрЃарЃЮ рЃбрЃћрЃЦрЃюрЃўрЃЎрЃБрЃарЃў рЃЏрЃўрЃЊрЃњрЃЮрЃЏрЃљ
- **Hugging Face Transformers**: Python-рЃЉрЃћрЃўрЃАрЃЊ рЃљрЃџрЃбрЃћрЃарЃюрЃљрЃбрЃўрЃЋрЃљ

## рЃЊрЃљрЃЏрЃљрЃбрЃћрЃЉрЃўрЃЌрЃў рЃарЃћрЃАрЃБрЃарЃАрЃћрЃЉрЃў

- [рЃЮрЃцрЃўрЃфрЃўрЃљрЃџрЃБрЃарЃў рЃЊрЃЮрЃЎрЃБрЃЏрЃћрЃюрЃбрЃљрЃфрЃўрЃљ](https://github.com/ollama/ollama)
- [рЃЏрЃЮрЃЊрЃћрЃџрЃћрЃЉрЃўрЃА рЃЉрЃўрЃЉрЃџрЃўрЃЮрЃЌрЃћрЃЎрЃљ](https://ollama.com/library)
- [Community Discord](https://discord.gg/ollama)
- [GitHub Issues](https://github.com/ollama/ollama/issues)

---

**рЃерЃћрЃюрЃўрЃерЃЋрЃюрЃљ**: Ollama рЃљрЃЦрЃбрЃўрЃБрЃарЃљрЃЊ рЃўрЃДрЃЏрЃћрЃюрЃћрЃЉрЃљ рЃЊрЃљ рЃљрЃ«рЃљрЃџрЃў рЃцрЃБрЃюрЃЦрЃфрЃўрЃћрЃЉрЃў рЃћрЃЏрЃљрЃбрЃћрЃЉрЃљ рЃарЃћрЃњрЃБрЃџрЃљрЃарЃБрЃџрЃљрЃЊ. рЃДрЃЮрЃЋрЃћрЃџрЃЌрЃЋрЃўрЃА рЃерЃћрЃљрЃЏрЃЮрЃгрЃЏрЃћрЃЌ рЃБрЃљрЃ«рЃџрЃћрЃАрЃў рЃЋрЃћрЃарЃАрЃўрЃљ рЃЊрЃљ рЃЊрЃЮрЃЎрЃБрЃЏрЃћрЃюрЃбрЃљрЃфрЃўрЃљ.